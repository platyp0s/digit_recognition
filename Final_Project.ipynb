# Final Project

## Optical Character Recognition via Supervised Machine Learning<br/>

#### In Optical Character Recognition, we want an algorithm that can work with any sort of image.
#### No matter how poor the hand writing, we want a chance at correctly guessing the character.
#### The algorithm itself should try to find the structure of the input character.
#### We can endeavor to accomplish this by having access to training data.
#### Based on that training data, the algorithm can make a determination on the classification of the image.
#### We can use the notion of a nearest neighbor classifier to determine which character an image is meant to represent.
#### http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification 
#### The 1,797 digit images in the datasets subset of the sklearn module are each represented by 8 by 8 pixels,<br/> meaning 64 pixels or dimensions, where each pixel has a darkness scale of 0 for lightest to 15 for darkest.
#### We can think of the individual pixels of each image as presenting a distance.
#### The distance from a pixel in one image to the corresponding pixel in another image can be based on the darkness or lightness of each pixel.  
#### A pixel of darkness 0 and the corresponding pixel of darkness 0 are closest since they contain the same value.
#### A pixel of darkness 15 and the corresponding pixel of darkness 0 are furthest, since they are at opposite ends of the spectrum.
#### If we determine an overall image 'distance' based on the corresponding pixel 'distances' between two images,<br/> we can determine how close they are to being the same character.
#### If we iterate over enough data, we can assert that the shortest 'distance' to a known character means that the <br/>unknown character should be the same as the charater to which it is nearest.

# allows for in-notebook graph/image generation
%matplotlib notebook
# used for plotting graphs
import matplotlib.pyplot as plt
# used for scientific computing
import numpy as np

# http://scikit-learn.org/stable/auto_examples/datasets/
# plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py
# sklearn datasets contains 1797 labeled images of digits
# Array digits.images contains the images
# Array digits.target contains the 'labels' for the images
# Each pixel from digits.images has a number between 0 and 15
# representing the darkness of each pixel
from sklearn import datasets
training_digits = datasets.load_digits()

# https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html
# http://stackoverflow.com/questions/1401712/
# how-can-the-euclidean-distance-be-calculated-with-numpy#1401828
def distance(x, y):
    ("This function returns the Euclidian distance between two points."
     "For example, for points x = [0, 0] and y = [3, 4], the function"
     "first subtracts [3, 4] from [0, 0], yielding point [-3, -4]."
     "The function then square that point to [9, 16] and sums the"
     "coordinates 9 + 16 = 25.  Finally, the square root"
     "of that value is taken: âˆš25 = 5."
     "Research suggested the function below as the fastest way of"
     "calculating Euclidean distance.")

    return np.linalg.norm(x - y)


# print the gray scale representation of the digit 9
print(training_digits.images[9])

# https://chrisalbon.com/python/set_the_color_of_a_matplotlib.html
# plot the image for digit 9
plt.figure()
plt.imshow(training_digits.images[9], cmap=plt.cm.binary)
plt.show()
# There does indeed appear to be a correspondence between
# the color of an individiual pixel and the number 0-15
# representing that pixel.

# We can also check if the image is indeed properly labeled.

print("Label of training_digits.images[9]: {}".format(training_digits.target[9]))

# the label is indeed 9
# Well, that is fine and dandy, but what does this have to do with machine learning
# and supervised learning?
# Good point.
# We can now choose a subset of datasets.load_digits(), which we have saved under
# the variable name training_digits, and create our training arrays from that subset.

# since the digits 0 through 9 repeat, making the training digits 0-9 makes sense
Train_digits = training_digits.data[0:10]
Train_labels = training_digits.target[0:10]

# We can choose a random number from the dataset and pretend we do not have its label.
Test_digit = training_digits.data[300]

# We can plot training_digits.data[300] to visually determine what it is
plt.figure()
plt.imshow(training_digits.images[300], cmap = plt.cm.binary)
plt.show()
# https://chrisalbon.com/python/set_the_color_of_a_matplotlib.html

# It seems to be a 7 but it sure it ugly.  Maybe a 3?
# We can test if our distance function and the data of the 
# 10 image subarray can correctly classify this 9.

#initialize an array of zeros
distances = np.zeros(len(Train_digits))
# calculate the distance between each digit and the test digit
for i in range(len(Train_digits)):
    distances[i] = distance(Train_digits[i], Test_digit)

# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html
# determine the index of the label with the closest distance
closest_label = np.argmin(distances)
# check the label determined based on the distances calculated
print(Train_labels[closest_label])

# Let us check the actual label of our test number to be sure.
print(training_digits.target[300])

# It seems the program did all right.
# But we really should test it against more than one image.
# After all, I myself could not tell if the image was a 7 or a 3 initially!
# Keeping track of the errors made would be ideal to determine the actual failure
# rate and to determine if there is improvement or degradation with larger
# data sets.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(0,100):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the percentage of correct predictions
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

# Wow, that is not so great.
# Only 77% correct.
# What can be done to get it better?
# Well, expanding the training set to more than 10 digits 
# would seem like a logical step.
# Let us double to 20.

Train_digits = training_digits.data[0:20]
Train_labels = training_digits.target[0:20]

# The remainder of the code should be the same.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(0,100):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the number of errors you made
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

# We have improvement!
# Maybe we should double the size of training set again 
# to see even more improvement?

Train_digits = training_digits.data[0:40]
Train_labels = training_digits.target[0:40]

# The remainder of the code should be the same.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(0,100):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the number of errors you made
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

# Even better!
# Maybe increasing the training set to 50 will yield no errors.

Train_digits = training_digits.data[0:50]
Train_labels = training_digits.target[0:50]

# The remainder of the code should be the same.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(0,100):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the number of errors you made
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

# Close to perfect!
# How about we increase to 100, but just to be sure,
# we test digits other than 0 through 99 and change
# the digits we want to test to say 100 through 199?

Train_digits = training_digits.data[0:100]
Train_labels = training_digits.target[0:100]

# The remainder of the code should be the same.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(100,200):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the number of errors you made
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

# Interesting!
# So even though we increased the training set
# to a high value, there are some characters
# we could not correctly predict in this new
# range of tested numbers.
# How about if we kick it up to 1000 and try 
# these same numbers from 100 to 199?

Train_digits = training_digits.data[0:1000]
Train_labels = training_digits.target[0:1000]

# The remainder of the code should be the same.

errors = 0
distances = np.zeros(len(Train_digits))

for i in range(100,200):
    Test_digit = training_digits.data[i]
    for j in range (len(Train_digits)):
        distances[j] = distance(Train_digits[j], Test_digit)
    closest_label = np.argmin(distances)
    if Train_labels[closest_label] != training_digits.target[i]:
        errors += 1

# print the number of errors you made
print(errors)
print("Correct percentage: {0:.2f}%".format((100 - errors)/ 100*100))

Perfection has been achieved!
So we have learned that we can teach the program
to calculate better by providing it with more
more and more training data, but we also learned
that a new batch of data can have show diminished
performace from the program, so the larget the 
training data set, the safer we are.
